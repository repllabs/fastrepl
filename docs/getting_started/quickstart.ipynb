{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"fastrepl==0.0.10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find all releases [here](https://pypi.org/project/fastrepl).\n",
    "\n",
    "## Setup FastREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastrepl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We will use [daily_dialog](https://huggingface.co/datasets/daily_dialog) from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"daily_dialog\", split=\"test\")\n",
    "ds = ds.shuffle(4)\n",
    "ds = ds.select(range(30))\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub(r\"\\s+([,.'!?])\", r\"\\1\", text.strip())\n",
    "\n",
    "\n",
    "def get_input(row):\n",
    "    msgs = [clean(msg) for msg in row[\"dialog\"]]\n",
    "    row[\"sample\"] = \"\\n\".join(msgs)  # `SimpleEvaluator` expect `sample` column\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "ds = ds.map(get_input, remove_columns=[\"dialog\", \"act\", \"emotion\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluator\n",
    "\n",
    "Here, we are doing simple classifiction, but there are two interesting points.\n",
    "\n",
    "1. You can pass nearly any model for evaluation. (Thanks to [LiteLLM](https://github.com/BerriAI/litellm)).\n",
    "2. **`fastrepl` enhances accuracy by reducing [bias](/guides/dealing_with_bias.md)**. `position_debias_strategy` is one example, which ensures that the order of labels doesn't affect the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = fastrepl.SimpleEvaluator(\n",
    "    node=fastrepl.LLMClassificationHead(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        context=\"You will receive casual conversation between two people.\",\n",
    "        labels={\n",
    "            \"FUN\": \"at least one of the two people try to be funny and entertain.\",\n",
    "            \"NOT_FUN\": \"given conversation lacks humor or entertainment value.\",\n",
    "        },\n",
    "        position_debias_strategy=\"consensus\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluator\n",
    "\n",
    "Here are some notes about running the evaluator:\n",
    "\n",
    "1. `ThreadPool` is used to make it faster (controlled by the `NUM_THREADS` [environment variable](/getting_started/env.md)).\n",
    "2. Any errors from different LLM providers are properly handled and retried with backoff if necessary.\n",
    "3. Since we passed `num=2` to `run()`, it will execute same evaluation twice, and return two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2e2ab62a9f4f708c77e159cd16ec48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample', 'result'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fastrepl.local_runner(evaluator=evaluator, dataset=ds).run(num=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting point to note is that, due to the `position_debias_strategy=\"consensus\"`, if the order of the labels affects the result, `fastrepl` will return `None`. We'll be returning more meaningful value in the later version of `fastrepl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print([row[0] for row in result[\"result\"]].count(None))\n",
    "print([row[1] for row in result[\"result\"]].count(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got some numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n",
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "def metric(result):\n",
    "    f = result.count(\"FUN\")\n",
    "    nf = result.count(\"NOT_FUN\")\n",
    "    return f / (f + nf)\n",
    "\n",
    "\n",
    "print(metric([row[0] for row in result[\"result\"]]))\n",
    "print(metric([row[1] for row in result[\"result\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fastrepl` has built-in support for measuring `inter-rater reliability` when `num > 1`. If `num = 2`, it use `Cohen's Kappa`, otherwise, it use `Fleiss' Kappa`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kappa': 0.686411149825784}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yujonglee/dev/fastrepl/fastrepl/fastrepl/warnings.py:25: CompletionTruncatedWarning: A | https://docs.fastrepl.com/miscellaneous/warnings_and_errors#completiontruncated\n"
     ]
    }
   ],
   "source": [
    "fastrepl.Analyzer(result).run(mode=\"kappa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got `0.7`, which means that the two raters have strong agreement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
