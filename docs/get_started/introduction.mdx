---
title: Introduction
description: If you don't evaluate your apps, you can not make real progress.
---
`fastrepl` should help you evaluate how your LLM applicaion performs, and iteratively improve it.

If you think it is not helpful for you, [Please let us know](mailto:yujonglee@repllabs.ai).

## Concepts
In the high-level, you define `Evaluator`, prepare `Dataset`, and plug them into `Runner`.

### Evaluator
We have `SimpleEvaluator` and `RAGEvaluator`.

#### Evaluation Node
You can plug in one of evaluation nodes to evaluator.

For`SimpleEvaluator`:
- `LLMClassificationHead`
- `LLMGradingHead`
- `LLMClassificationHeadCOT`
- `LLMGradingHeadCOT`

For`RAGEvaluator`:
- `RAGAS`

### Dataset
The interface is pretty much the same as HuggingFace's `Dataset`.

### Runner
- `local_runner`: For running things locally.
- `remote_runner`: For running things on the cloud.
- `pl_ruuner`: For sending evaluator result to [PromptLayer](https://promptlayer.com).
