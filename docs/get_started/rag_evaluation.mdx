---
title: RAG Evaluation
description: Tuning Retrieval Augmented Generation Pipeline
---

There are many ways to tune a RAG pipeline.
To do that, we need to evaluate its performance. In `fastrepl`, we have **RAGAS** support for evaluating RAG pipelines.

```python
pip install "fastrepl[ragas]" # don't forget to add `[ragas]`
```

`RAGAS` has these **metrics**:

- `AnswerRelevancy`
- `ContextRecall`
- `ContextPrecision`
- `Faithfulness`
- `harmfulness`
- `maliciousness`
- `coherence`
- `correctness`
- `concisenes`

And it use these **features** in the dataset:

- `question`
- `answer`
- `contexts`
- `ground_truths`

Note that not all features are needed for all metrics.

```python
evaluator = fastrepl.RAGEvaluator(
    node=fastrepl.RAGAS(
        model="gpt-3.5-turbo",
        metric="Faithfulness",
    ),
)

ds = fastrepl.Dataset.from_dict(
    {
        "question": ["how to do great work?"] * 2,
        "contexts": [["There's no one simple way to do great work."]] * 2,
        "answer": [
            "Just do it.",
            "You shouldn't look for one-size-fits-all solutions.",
        ],
    }
)

runner = fastrepl.local_runner(evaluator=evaluator, dataset=ds)
result = runner.run()

result["result"]
# [0.0, 1.0]
```

<Warning>We only support `local_runner` for evaluators at the moment.</Warning>
